{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as p\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(data_dir):\n",
    "    train_data_path = os.path.join(data_dir, 'train-images.idx3-ubyte')\n",
    "    train_labels_path = os.path.join(data_dir, 'train-labels.idx1-ubyte')\n",
    "    test_data_path = os.path.join(data_dir, 't10k-images.idx3-ubyte')\n",
    "    test_labels_path = os.path.join(data_dir, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "\n",
    "    # Load training images\n",
    "    with open(train_data_path, 'rb') as f:\n",
    "        magic, num_images, rows, cols = np.fromfile(f, dtype=np.dtype('>i4'), count=4)\n",
    "        train_data = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "\n",
    "    # Load training labels\n",
    "    with open(train_labels_path, 'rb') as f:\n",
    "        magic, num_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=2)\n",
    "        train_labels = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "    # Load test images\n",
    "    with open(test_data_path, 'rb') as f:\n",
    "        magic, num_images, rows, cols = np.fromfile(f, dtype=np.dtype('>i4'), count=4)\n",
    "        test_data = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "\n",
    "    # Load test labels\n",
    "    with open(test_labels_path, 'rb') as f:\n",
    "        magic, num_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=2)\n",
    "        test_labels = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "    # Convert images to torch tensors and apply augmentation transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Normalizing the test images\n",
    "    transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "   \n",
    "    train_data = torch.stack([transform_train(img) for img in train_data])\n",
    "    test_data = torch.stack([transform_test(img) for img in test_data])\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_images, train_labels, test_images, test_labels \u001b[38;5;241m=\u001b[39m load_mnist_data(dataset_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print shapes of train_images, train_labels, test_images, test_labels\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_images\u001b[38;5;241m.\u001b[39mshape, train_labels\u001b[38;5;241m.\u001b[39mshape, test_images\u001b[38;5;241m.\u001b[39mshape, test_labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[24], line 45\u001b[0m, in \u001b[0;36mload_mnist_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     38\u001b[0m transform_test \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     39\u001b[0m transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[1;32m     40\u001b[0m transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     41\u001b[0m transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.4914\u001b[39m, \u001b[38;5;241m0.4822\u001b[39m, \u001b[38;5;241m0.4465\u001b[39m), (\u001b[38;5;241m0.2023\u001b[39m, \u001b[38;5;241m0.1994\u001b[39m, \u001b[38;5;241m0.2010\u001b[39m)),\n\u001b[1;32m     42\u001b[0m ])\n\u001b[1;32m     44\u001b[0m train_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([transform_train(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[0;32m---> 45\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([transform_test(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_data, train_labels, test_data, test_labels\n",
      "Cell \u001b[0;32mIn[24], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m transform_test \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     39\u001b[0m transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[1;32m     40\u001b[0m transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     41\u001b[0m transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.4914\u001b[39m, \u001b[38;5;241m0.4822\u001b[39m, \u001b[38;5;241m0.4465\u001b[39m), (\u001b[38;5;241m0.2023\u001b[39m, \u001b[38;5;241m0.1994\u001b[39m, \u001b[38;5;241m0.2010\u001b[39m)),\n\u001b[1;32m     42\u001b[0m ])\n\u001b[1;32m     44\u001b[0m train_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([transform_train(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[0;32m---> 45\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([transform_test(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_data, train_labels, test_data, test_labels\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:349\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mnormalize(tensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd, inplace\u001b[38;5;241m=\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py:926\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    925\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msub_(mean)\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]"
     ]
    }
   ],
   "source": [
    "dataset_path = './dataset'\n",
    "train_images, train_labels, test_images, test_labels = load_mnist_data(dataset_path)\n",
    "\n",
    "#print shapes of train_images, train_labels, test_images, test_labels\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "\n",
    "# Create DataLoader objects for images and labels\n",
    "train_images_dataset = TensorDataset(train_images, torch.tensor(train_labels))\n",
    "test_images_dataset = TensorDataset(test_images, torch.tensor(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_images_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_images_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any data processing we need to do here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweakable hyperparameters for the model\n",
    "# loss function and optimizer are placed in the training cell\n",
    "batch_size = 32\n",
    "num_of_epochs = 300\n",
    "learning_rate = 0.005\n",
    "neurons_per_layer = 128 # for the hidden layers\n",
    "\n",
    "numbers_of_layers = 3 # num of hidden layers excl. the input and output layers\n",
    "activation_function = torch.nn.Sigmoid() # try other activation functions too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model\n",
    "class DigitClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.all_layers = torch.nn.Sequential()\n",
    "\n",
    "        # input layer\n",
    "        self.all_layers.add_module('input', torch.nn.Linear(num_features, neurons_per_layer))\n",
    "        self.all_layers.add_module('input_activation', activation_function)\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(numbers_of_layers):\n",
    "            self.all_layers.add_module(f'hidden_{i}', torch.nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            self.all_layers.add_module(f'hidden_{i}_activation', activation_function)\n",
    "\n",
    "        # output layer\n",
    "        self.all_layers.add_module('output', torch.nn.Linear(neurons_per_layer, num_classes))\n",
    "        self.all_layers.add_module('output_activation', torch.nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.all_layers(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ignore for now##########\n",
    "# Data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#TODO: implement our dataloader, the commented lines are suggested by Copilot but I'm not sure if they are correct\n",
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        pass\n",
    "        # self.images = images\n",
    "        # self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "        # return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "        # return self.images[idx], self.labels[idx]\n",
    "    \n",
    "train_dataset = DigitsDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training loop\n",
    "\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(555)\n",
    "\n",
    "# dimension n of the n*n input picture\n",
    "n = 28\n",
    "\n",
    "# number of classes (digits 0-9)\n",
    "num_classes = 10\n",
    "\n",
    "# the model\n",
    "model = DigitClassifier(n*n, num_classes)\n",
    "\n",
    "# loss and optimizer, try other combos too?\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train!\n",
    "num_epochs = num_of_epochs\n",
    "epoch_losses = [] # to plot the loss curve later\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(features)\n",
    "        loss = F.mse_loss(logits, labels) # Loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # logging + save the loss\n",
    "    print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\" f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\" f\" | Train/Val Loss: {loss:.2f}\")\n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "# save model after training\n",
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loss plot\n",
    "plt.plot(epoch_losses)\n",
    "plt.legend([\"Training Loss\"])\n",
    "plt.title(\"Training Loss vs. Number of Epoch\")\n",
    "plt.xlabel(\"Number of Epoch\")\n",
    "plt.ylabel(\"Training Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

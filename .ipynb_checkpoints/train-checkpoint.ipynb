{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as p\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "Load the datasets 'train images', 'train labels', 'test images', and 'test labels' that have been downloaded and saved in ./datasets. The dataset is a sub-dataset of MNIST including 28*28 greyscale images of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(data_dir):\n",
    "    train_data_path = os.path.join(data_dir, 'train-images.idx3-ubyte')\n",
    "    train_labels_path = os.path.join(data_dir, 'train-labels.idx1-ubyte')\n",
    "    test_data_path = os.path.join(data_dir, 't10k-images.idx3-ubyte')\n",
    "    test_labels_path = os.path.join(data_dir, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "\n",
    "    # Load training images\n",
    "    with open(train_data_path, 'rb') as f:\n",
    "        magic, num_images, rows, cols = np.fromfile(f, dtype=np.dtype('>i4'), count=4)\n",
    "        train_data = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "\n",
    "    # Load training labels\n",
    "    with open(train_labels_path, 'rb') as f:\n",
    "        magic, num_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=2)\n",
    "        train_labels = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "    # Load test images\n",
    "    with open(test_data_path, 'rb') as f:\n",
    "        magic, num_images, rows, cols = np.fromfile(f, dtype=np.dtype('>i4'), count=4)\n",
    "        test_data = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "\n",
    "    # Load test labels\n",
    "    with open(test_labels_path, 'rb') as f:\n",
    "        magic, num_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=2)\n",
    "        test_labels = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "    # Convert images to torch tensors and apply augmentation transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Normalizing the test images\n",
    "    transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "   \n",
    "    train_data = torch.stack([transform_train(img) for img in train_data])\n",
    "    test_data = torch.stack([transform_test(img) for img in test_data])\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) (60000,) torch.Size([10000, 1, 28, 28]) (10000,)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './dataset'\n",
    "train_images, train_labels, test_images, test_labels = load_mnist_data(dataset_path)\n",
    "\n",
    "#print shapes of train_images, train_labels, test_images, test_labels\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "\n",
    "# Create DataLoader objects for images and labels\n",
    "train_images_dataset = TensorDataset(train_images, torch.tensor(train_labels, dtype=torch.long))\n",
    "test_images_dataset = TensorDataset(test_images, torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_images_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_images_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any data processing we need to do here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweakable hyperparameters for the model\n",
    "# loss function and optimizer are placed in the training cell\n",
    "batch_size = 32\n",
    "num_of_epochs = 10\n",
    "learning_rate = 0.001\n",
    "neurons_per_layer = 128 # for the hidden layers\n",
    "\n",
    "numbers_of_layers = 3 # num of hidden layers excl. the input and output layers\n",
    "activation_function = torch.nn.Sigmoid() # try other activation functions too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "First model to try is a 3 layer neural network model with ReLU activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model\n",
    "class DigitClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        # input layer\n",
    "    ##    self.all_layers.add_module('input', torch.nn.Linear(num_features, neurons_per_layer))\n",
    "      ##  self.all_layers.add_module('input_activation', activation_function)\n",
    "\n",
    "        # hidden layers\n",
    "      ##  for i in range(numbers_of_layers):\n",
    "      ##      self.all_layers.add_module(f'hidden_{i}', torch.nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "      ##      self.all_layers.add_module(f'hidden_{i}_activation', activation_function)\n",
    "\n",
    "        # output layer\n",
    "     ##   self.all_layers.add_module('output', torch.nn.Linear(neurons_per_layer, num_classes))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.classifier(x)    \n",
    "        return x\n",
    "    \n",
    "# the model\n",
    "#grey scale, num_channel=1\n",
    "model = DigitClassifier(28*28*1, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model to try is a mobilenetV2. MobileNetV2 is an improvement over the original MobileNet architecture, introduced in the paper \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" by Sandler et al. It introduces inverted residuals with linear bottlenecks and other architectural changes to improve the efficiency and performance of MobileNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MobileNetV2.forward() got an unexpected keyword argument 'pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmobilenet(x)\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m ModifiedMobileNetV2(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m, in \u001b[0;36mModifiedMobileNetV2.__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(ModifiedMobileNetV2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmobilenet \u001b[38;5;241m=\u001b[39m mobilenet_v2(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmobilenet\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize \u001b[38;5;241m=\u001b[39m Resize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)) \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: MobileNetV2.forward() got an unexpected keyword argument 'pretrained'"
     ]
    }
   ],
   "source": [
    "######ignore for now ###########\n",
    "import torchvision.models as models\n",
    "class ModifiedMobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedMobileNetV2, self).__init__()\n",
    "        self.mobilenet = mobilenet_v2(pretrained=False)\n",
    "        self.mobilenet.features[0][0] = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.resize = Resize((224, 224)) \n",
    "        # Modify the classifier\n",
    "        self.mobilenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1280, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)\n",
    "    \n",
    "model = ModifiedMobileNetV2(num_classes=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third model to try\n",
    "#4 layers cnn with ReLU activation\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Adjust the input size according to your image dimensions\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten the feature maps for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 1874/1875 | Train Loss: 0.33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#print(\"Shape of target tensor:\", labels.shape)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print(logits.shape)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels) \u001b[38;5;66;03m# Loss function\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# logging + save the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the training loop\n",
    "\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(555)\n",
    "\n",
    "# dimension n of the n*n input MNIST image\n",
    "n = 28\n",
    "\n",
    "# number of classes (digits 0-9)\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "\n",
    "# loss and optimizer, try other combos too?\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#also try Adam\n",
    "\n",
    "# train!\n",
    "num_epochs = 10   #increase this to at least 50\n",
    "epoch_losses = [] # to plot the loss curve later\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        #use this version for mobilenet\n",
    "        images_rgb = torch.cat((images, images, images), dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        #print(\"Shape of target tensor:\", labels.shape)\n",
    "        #print(logits.shape)\n",
    "        loss = loss_fn(logits, labels) # Loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # logging + save the loss\n",
    "    print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\" f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\" f\" | Train Loss: {loss:.2f}\")\n",
    "    epoch_losses.append(loss)\n",
    "\n",
    "\n",
    "# save model after training\n",
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loss plot\n",
    "plt.plot(epoch_losses)\n",
    "plt.legend([\"Training Loss\"])\n",
    "plt.title(\"Training Loss vs. Number of Epoch\")\n",
    "plt.xlabel(\"Number of Epoch\")\n",
    "plt.ylabel(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = []\n",
    "total_loss = 0\n",
    "for epochs in range(num_epochs)\n",
    "    model.eval()\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader)\n",
    "        out = model(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()  \n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "    # logging + save the loss\n",
    "    print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\" f\" | Batch {batch_idx:03d}/{len(test_loader):03d}\" f\" | Validation Loss: {loss:.2f}\")\n",
    "    val_loss.append(avg_loss)\n",
    "    \n",
    "    \n",
    "#add some more measurements here: accuracy, confusion matrix, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
